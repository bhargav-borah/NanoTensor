{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "FoNq4M4bhtVS"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile engine.py\n",
        "class Value:\n",
        "\n",
        "  def __init__(self, data, _children=(), _op='', label=''):\n",
        "    self.data = data\n",
        "    self.grad = 0.0\n",
        "    self._backward = lambda: None\n",
        "    self._prev = set(_children)\n",
        "    self._op = _op\n",
        "    self.label = label\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f'Value(data={self.data})'\n",
        "\n",
        "  def __add__(self, other):\n",
        "    other = other if isinstance(other, Value) else Value(other)\n",
        "    out = Value(self.data + other.data, (self, other), '+')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += 1.0 * out.grad\n",
        "      other.grad += 1.0 * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def __radd__(self, other):\n",
        "    return self + other\n",
        "\n",
        "  def __neg__(self):\n",
        "    return self * -1\n",
        "\n",
        "  def __sub__(self, other):\n",
        "    return self + (-other)\n",
        "\n",
        "  def __mul__(self, other):\n",
        "    other = other if isinstance(other, Value) else Value(other)\n",
        "    out = Value(self.data * other.data, (self, other), '*')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += other.data * out.grad\n",
        "      other.grad += self.data * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def __rmul__(self, other):\n",
        "    return self * other\n",
        "\n",
        "  def __truediv__(self, other):\n",
        "    return self * (other**-1)\n",
        "\n",
        "  def __pow__(self, other):\n",
        "    assert isinstance(other, (int, float)), 'Only supporting int/float powers for now'\n",
        "    out = Value(self.data**other, (self,), f'**{other}')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += other * (self.data**(other - 1)) * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def exp(self):\n",
        "    x = self.data\n",
        "    out = Value(math.exp(x), (self,), 'exp')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += out.data * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def tanh(self):\n",
        "    x = self.data\n",
        "    t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n",
        "    out = Value(t, (self,), 'tanh')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += (1 - t**2) * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def relu(self):\n",
        "    x = self.data\n",
        "    out = Value(0 if x < 0 else x, (self,), 'relu')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += (out.data > 0) * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def sigmoid(self):\n",
        "    x = self.data\n",
        "    x = max(min(x, 700), -700)  # Clamping to avoid overflow\n",
        "    s = 1 / (1 + math.exp(-x))\n",
        "    out = Value(s, (self,), 'sigmoid')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += s * (1 - s) * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def elu(self, alpha=1.0):\n",
        "    x = self.data\n",
        "    out = Value(x if x > 0 else alpha * (math.exp(x) - 1), (self,), 'elu')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += (1 if x > 0 else alpha * math.exp(x)) * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def hard_shrink(self, _lambda=0.5):\n",
        "    x = self.data\n",
        "    out = Value(x * (x > _lambda) + x * (x < -_lambda), (self,), 'hard_shrink')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += ((x > _lambda) + (x < _lambda)) * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def hard_sigmoid(self):\n",
        "    x = self.data\n",
        "    out = Value((x >= 3) + ((x / 6) + 0.5) * (x > -3 and x < 3), (self,), 'hard_sigmoid')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += ((1 / 6) * (x > -3 and x < 3)) * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def hard_tanh(self, min_val=-1.0, max_val=1.0):\n",
        "    x = self.data\n",
        "    out = Value(max_val * (x > max_val) + min_val * (x < min_val) + x * (x >= min_val and x <= max_val), (self,), 'hard_tanh')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += (x >= min_val and x <= max_val) * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def hardswish(self):\n",
        "    x = self.data\n",
        "    out = Value(x * (x >= 3) + (x * (x + 3) / 6) * (x > -3 and x < 3), (self,), 'hardswish')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += ((x >= 3) + ((2 * x + 3) / 6) * (x > -3 and x < 3)) * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def leaky_relu(self, negative_slope=0.01):\n",
        "    x = self.data\n",
        "    out = Value(x if (x >= 0) else negative_slope * x, (self,), 'leaky_relu')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += (1 if (x >= 0) else negative_slope) * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def log_sigmoid(self):\n",
        "    x = self.data\n",
        "    s = 1 / (1 + math.exp(-x))\n",
        "    out = Value(math.log(s), (self,), 'log_sigmoid')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += (1 - s) * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def relu_6(self):\n",
        "    x = min(max(self.data, 0), 6)\n",
        "    out = Value(x, (self,), 'relu_6')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += (x > 0 and x < 6) * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def rrelu(self, lower=0.25, upper=0.3333333333333333, training=True):\n",
        "    if training:\n",
        "      self.slope = random.uniform(lower, upper)\n",
        "    else:\n",
        "      self.slope = (lower + upper) / 2\n",
        "\n",
        "    x = self.data\n",
        "    out = Value(x if x > 0 else self.slope * x, (self,), 'rrelu')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += (1 if x > 0 else self.slope) * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def selu(self):\n",
        "    scale = 1.0507009873554804934193349852946\n",
        "    alpha = 1.6732632423543772848170429916717\n",
        "    x = scale * (max(0, self.data) + min(0, alpha * (math.exp(self.data) - 1)))\n",
        "    out = Value(x, (self,), 'selu')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += (scale * (self.data > 0) + (x <= 0) * (scale * alpha * math.exp(x))) * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def celu(self, alpha=1.0):\n",
        "    x = max(0, self.data) + min(0, alpha * (math.exp(self.data / alpha) - 1))\n",
        "    out = Value(x, (self,), 'celu')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += (1 if self.data > 0 else math.exp(self.data / alpha)) * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def gelu(self):\n",
        "    x = self.data\n",
        "    out = Value(0.5 * x * (1 + math.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * x**3))), (self,), 'gelu')\n",
        "\n",
        "    def _backward():\n",
        "        sech_squared = 4 / ((math.exp(math.sqrt(2 / math.pi) * (x + 0.044715 * x**3)) + math.exp(-math.sqrt(2 / math.pi) * (x + 0.044715 * x**3))) ** 2)\n",
        "        self.grad += sech_squared * (math.sqrt(2 / math.pi)) * (1 + 3 * 0.044715 * x**2) * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def silu(self):\n",
        "    x = self.data\n",
        "    s = 1 / (1 + math.exp(-x))\n",
        "    out = Value(x * s, (self,), 'silu')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += s * (1 + x * (1 - s)) * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def softplus(self, beta=1.0, threshold=20.0):\n",
        "    x = self.data\n",
        "    out_val = x if x > threshold else (1 / beta) * math.log(1 + math.exp(beta * x))\n",
        "    out = Value(out_val, (self,), 'softplus')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += (1 if x > threshold else (1 / (1 + math.exp(-beta * x)))) * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def mish(self, beta=1.0):\n",
        "    x = self.data\n",
        "    softplus_out = (1 / beta) * math.log(1 + math.exp(beta * x))\n",
        "    out_val = x * math.tanh(softplus_out)\n",
        "    out = Value(out_val, (self,), 'mish')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += math.tanh(softplus_out) + x * (1 / (1 + math.exp(-x))) * (1 - math.tanh(softplus_out) ** 2) * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def softshrink(self, _lambda=0.5):\n",
        "    x = self.data\n",
        "    out = Value((x - _lambda) * (x > _lambda) + (x + _lambda) * (x < -_lambda), (self,), 'softshrink')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += ((x > _lambda) + (x < -_lambda)) * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def softsign(self):\n",
        "    x = self.data\n",
        "    out = Value(x / (1 + abs(x)), (self,), 'softsign')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += (1 / (1 + abs(x)) ** 2) * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def tanhshrink(self):\n",
        "    x = self.data\n",
        "    out = Value(x - math.tanh(x), (self,), 'tanhshrink')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += (math.tanh(x) ** 2) * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self):\n",
        "\n",
        "    topo = []\n",
        "    visited = set()\n",
        "\n",
        "    def build_topo(v):\n",
        "      if v not in visited:\n",
        "        visited.add(v)\n",
        "        for child in v._prev:\n",
        "          build_topo(child)\n",
        "        topo.append(v)\n",
        "    build_topo(self)\n",
        "\n",
        "    self.grad = 1.0\n",
        "    for node in reversed(topo):\n",
        "      node._backward()"
      ],
      "metadata": {
        "id": "YKrH_f7EqkfD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6380ac7-d9ea-461c-8e35-490f0bd148c1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a engine.py\n",
        "from graphviz import Digraph\n",
        "\n",
        "def trace(root):\n",
        "  # builds a set of all nodes and edges in a graph\n",
        "  nodes, edges = set(), set()\n",
        "  def build(v):\n",
        "    if v not in nodes:\n",
        "      nodes.add(v)\n",
        "      for child in v._prev:\n",
        "        edges.add((child, v))\n",
        "        build(child)\n",
        "  build(root)\n",
        "  return nodes, edges\n",
        "\n",
        "def draw_dot(root):\n",
        "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
        "\n",
        "  nodes, edges = trace(root)\n",
        "  for n in nodes:\n",
        "    uid = str(id(n))\n",
        "    # for any value in the graph, create a rectangular ('record') node for it\n",
        "    dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n",
        "    if n._op:\n",
        "      # if this value is a result of some operation, create an op node for it\n",
        "      dot.node(name = uid + n._op, label = n._op)\n",
        "      # and connect this node to it\n",
        "      dot.edge(uid + n._op, uid)\n",
        "\n",
        "  for n1, n2 in edges:\n",
        "    # connect n1 to the op node of n2\n",
        "    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
        "\n",
        "  return dot"
      ],
      "metadata": {
        "id": "BDa1kbPttcw5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b55e6bb-2cdc-43bc-e62a-6d8ef0ef0a0e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a engine.py\n",
        "class Neuron:\n",
        "\n",
        "    def __init__(self, nin):\n",
        "        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
        "        self.b = Value(random.uniform(-1, 1))\n",
        "\n",
        "    def __call__(self, x, activation=None):\n",
        "        # w * x + b\n",
        "        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n",
        "\n",
        "        if activation is None or activation == 'linear':\n",
        "          out = act\n",
        "        elif activation == 'relu':\n",
        "          out = act.relu()\n",
        "        elif activation == 'tanh':\n",
        "          out = act.tanh()\n",
        "        elif activation == 'sigmoid':\n",
        "          out = act.sigmoid()\n",
        "        elif activation == 'elu':\n",
        "          out = act.elu()\n",
        "        elif activation == 'hard_shrink':\n",
        "          out = act.hard_shrink()\n",
        "        elif activation == 'hard_sigmoid':\n",
        "          out = act.hard_sigmoid()\n",
        "        elif activation == 'hard_tanh':\n",
        "          out = act.hard_tanh()\n",
        "        elif activation == 'hardswish':\n",
        "          out = act.hardswish()\n",
        "        elif activation == 'leaky_relu':\n",
        "          out = act.leaky_relu()\n",
        "        elif activation == 'log_sigmoid':\n",
        "          out = act.log_sigmoid()\n",
        "        elif activation == 'relu_6':\n",
        "          out = act.relu_6()\n",
        "        elif activation == 'rrelu':\n",
        "          out = act.rrelu()\n",
        "        elif activation == 'selu':\n",
        "          out = act.selu()\n",
        "        elif activation == 'celu':\n",
        "          out = act.celu()\n",
        "        elif activation == 'gelu':\n",
        "          out = act.gelu()\n",
        "        elif activation == 'silu':\n",
        "          out = act.silu()\n",
        "        elif activation == 'softplus':\n",
        "          out = act.softplus()\n",
        "        elif activation == 'mish':\n",
        "          out = act.mish()\n",
        "        elif activation == 'softshrink':\n",
        "          out = act.softshrink()\n",
        "        elif activation == 'softsign':\n",
        "          out = act.softsign()\n",
        "        elif activation == 'tanhshrink':\n",
        "          out = act.tanhshrink()\n",
        "\n",
        "        return out\n",
        "\n",
        "    def parameters(self):\n",
        "      return self.w + [self.b]\n",
        "\n",
        "class Layer:\n",
        "\n",
        "    def __init__(self, nin, nout, activation):\n",
        "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
        "\n",
        "    def __call__(self, x, activation):\n",
        "        outs = [n(x, activation=activation) for n in self.neurons]\n",
        "        return outs\n",
        "\n",
        "    def parameters(self):\n",
        "      # return [p for neuron in self.neurons for p in neuron.parameters()]\n",
        "      params = []\n",
        "      for neuron in self.neurons:\n",
        "        params.extend(neuron.parameters())\n",
        "\n",
        "      return params\n",
        "\n",
        "class MLP:\n",
        "\n",
        "    def __init__(self, nin, nouts, activations):\n",
        "        sz = [nin] + nouts\n",
        "        self.layers = [Layer(sz[i], sz[i+1], activation=activations[i]) for i in range(len(nouts))]\n",
        "        self.activations = activations\n",
        "\n",
        "    def __call__(self, x):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x, activation=self.activations[i])\n",
        "        return x if len(x) > 1 else x[0]\n",
        "\n",
        "    def parameters(self):\n",
        "      return [p for layer in self.layers for p in layer.parameters()]\n",
        "\n",
        "    def fit(self, xs, ys, n_epochs=1000, verbose=False):\n",
        "      for epoch in range(n_epochs):\n",
        "        ypred = [self(x) for x in xs]\n",
        "        loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\n",
        "\n",
        "        for p in self.parameters():\n",
        "          p.grad = 0.0\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        for p in self.parameters():\n",
        "          p.data -= 0.01 * p.grad\n",
        "\n",
        "        if verbose:\n",
        "          print(f'Epoch {epoch + 1}, Loss: {loss.data}')"
      ],
      "metadata": {
        "id": "5T0ncJesGqkm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0377889-8b6b-4034-c890-d9006112b736"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = [2.0, 3.0, -1.0]\n",
        "n = MLP(3, [4, 4, 1], ['relu', 'relu', None])\n",
        "output = n(x)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUWdtN3YT3Sd",
        "outputId": "1f2c71b4-9495-4a45-aab9-26986f65926a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value(data=1.859415124183045)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xs = [\n",
        "    [2.0, 3.0, -1.0],\n",
        "    [3.0, -1.0, 0.5],\n",
        "    [0.5, 1.0, 1.0],\n",
        "    [1.0, 1.0, -1.0]\n",
        "]\n",
        "ys = [1.0, -1.0, -1.0, 1.0]"
      ],
      "metadata": {
        "id": "HyEi8SfKOwbh"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(1000):\n",
        "#   ypred = [n(x) for x in xs]\n",
        "#   loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\n",
        "\n",
        "#   for p in n.parameters():\n",
        "#     p.grad = 0.0\n",
        "\n",
        "#   loss.backward()\n",
        "\n",
        "#   for p in n.parameters():\n",
        "#     p.data -= 0.01 * p.grad\n",
        "\n",
        "#   print(i + 1, loss.data)\n",
        "n.fit(xs, ys, verbose=True)"
      ],
      "metadata": {
        "id": "-KZV1P66Ux9p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4608f259-f6f7-4765-8176-b99c2aa4b2c5"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 10.430304850912393\n",
            "Epoch 2, Loss: 5.084775542207613\n",
            "Epoch 3, Loss: 4.52773813574073\n",
            "Epoch 4, Loss: 4.046415562032458\n",
            "Epoch 5, Loss: 3.6680007551399347\n",
            "Epoch 6, Loss: 3.3602562223475902\n",
            "Epoch 7, Loss: 3.1097942367601066\n",
            "Epoch 8, Loss: 2.888770887214672\n",
            "Epoch 9, Loss: 2.697362776269872\n",
            "Epoch 10, Loss: 2.521807165707247\n",
            "Epoch 11, Loss: 2.4139686249844376\n",
            "Epoch 12, Loss: 2.3323608289337985\n",
            "Epoch 13, Loss: 2.2655667327979834\n",
            "Epoch 14, Loss: 2.2013349557727477\n",
            "Epoch 15, Loss: 2.1351901490356493\n",
            "Epoch 16, Loss: 2.0654358053868362\n",
            "Epoch 17, Loss: 1.9961561028300552\n",
            "Epoch 18, Loss: 1.9317973364078675\n",
            "Epoch 19, Loss: 1.8640914664288102\n",
            "Epoch 20, Loss: 1.7866587892676316\n",
            "Epoch 21, Loss: 1.7263108033988843\n",
            "Epoch 22, Loss: 1.6474303892828357\n",
            "Epoch 23, Loss: 1.5679650354248549\n",
            "Epoch 24, Loss: 1.5051759095637212\n",
            "Epoch 25, Loss: 1.4330436167455898\n",
            "Epoch 26, Loss: 1.3699378831297904\n",
            "Epoch 27, Loss: 1.3190288385976143\n",
            "Epoch 28, Loss: 1.2534608221550698\n",
            "Epoch 29, Loss: 1.1943577212798422\n",
            "Epoch 30, Loss: 1.1414601138757658\n",
            "Epoch 31, Loss: 1.0803202327634138\n",
            "Epoch 32, Loss: 1.0236254297384586\n",
            "Epoch 33, Loss: 0.9675917303698791\n",
            "Epoch 34, Loss: 0.9106176579533312\n",
            "Epoch 35, Loss: 0.854990591776692\n",
            "Epoch 36, Loss: 0.8004034434257162\n",
            "Epoch 37, Loss: 0.744888092613944\n",
            "Epoch 38, Loss: 0.6947583319485586\n",
            "Epoch 39, Loss: 0.6445542196281686\n",
            "Epoch 40, Loss: 0.5919071522737271\n",
            "Epoch 41, Loss: 0.5448880993853562\n",
            "Epoch 42, Loss: 0.5030704815680902\n",
            "Epoch 43, Loss: 0.45560129665816956\n",
            "Epoch 44, Loss: 0.4152249078352407\n",
            "Epoch 45, Loss: 0.37544445107031954\n",
            "Epoch 46, Loss: 0.3404837503643838\n",
            "Epoch 47, Loss: 0.3063801899007185\n",
            "Epoch 48, Loss: 0.2714504698578504\n",
            "Epoch 49, Loss: 0.2426688212800999\n",
            "Epoch 50, Loss: 0.21713996730591992\n",
            "Epoch 51, Loss: 0.19079442708467056\n",
            "Epoch 52, Loss: 0.16688318339804192\n",
            "Epoch 53, Loss: 0.147865340764188\n",
            "Epoch 54, Loss: 0.1292247561588255\n",
            "Epoch 55, Loss: 0.11396784198103585\n",
            "Epoch 56, Loss: 0.10007310340050274\n",
            "Epoch 57, Loss: 0.0869607437530876\n",
            "Epoch 58, Loss: 0.07532551901859184\n",
            "Epoch 59, Loss: 0.0660512716652703\n",
            "Epoch 60, Loss: 0.05741818707005471\n",
            "Epoch 61, Loss: 0.050178825216617526\n",
            "Epoch 62, Loss: 0.04413675354787339\n",
            "Epoch 63, Loss: 0.03839357368693565\n",
            "Epoch 64, Loss: 0.03345867606449085\n",
            "Epoch 65, Loss: 0.0293744750944285\n",
            "Epoch 66, Loss: 0.02571072797611035\n",
            "Epoch 67, Loss: 0.022661609298750274\n",
            "Epoch 68, Loss: 0.02003090514390382\n",
            "Epoch 69, Loss: 0.017677095707686833\n",
            "Epoch 70, Loss: 0.015652534891744518\n",
            "Epoch 71, Loss: 0.013944973502823234\n",
            "Epoch 72, Loss: 0.01242004846732357\n",
            "Epoch 73, Loss: 0.011113749617564846\n",
            "Epoch 74, Loss: 0.009978290128858687\n",
            "Epoch 75, Loss: 0.008975149017672221\n",
            "Epoch 76, Loss: 0.008091934592127077\n",
            "Epoch 77, Loss: 0.007303998602141414\n",
            "Epoch 78, Loss: 0.006609825688052627\n",
            "Epoch 79, Loss: 0.005997275042420468\n",
            "Epoch 80, Loss: 0.005450690658791774\n",
            "Epoch 81, Loss: 0.0049639256019389155\n",
            "Epoch 82, Loss: 0.00452723590149156\n",
            "Epoch 83, Loss: 0.004135187777242872\n",
            "Epoch 84, Loss: 0.0037816262570198335\n",
            "Epoch 85, Loss: 0.0034623105691661974\n",
            "Epoch 86, Loss: 0.0031731387911018497\n",
            "Epoch 87, Loss: 0.0029106263653037676\n",
            "Epoch 88, Loss: 0.0026718083403541827\n",
            "Epoch 89, Loss: 0.0024541365063567027\n",
            "Epoch 90, Loss: 0.0022554147293124827\n",
            "Epoch 91, Loss: 0.0020737339771131685\n",
            "Epoch 92, Loss: 0.0019074289188378227\n",
            "Epoch 93, Loss: 0.0017550361088730722\n",
            "Epoch 94, Loss: 0.0016152646706561516\n",
            "Epoch 95, Loss: 0.0014869689352488493\n",
            "Epoch 96, Loss: 0.001369128453219861\n",
            "Epoch 97, Loss: 0.001260829755622124\n",
            "Epoch 98, Loss: 0.00116125253726922\n",
            "Epoch 99, Loss: 0.0010696572336689842\n",
            "Epoch 100, Loss: 0.0009853752970388212\n",
            "Epoch 101, Loss: 0.0009078005252385414\n",
            "Epoch 102, Loss: 0.0008363820706260021\n",
            "Epoch 103, Loss: 0.0007706182253547436\n",
            "Epoch 104, Loss: 0.0007100512761644463\n",
            "Epoch 105, Loss: 0.0006542629273987861\n",
            "Epoch 106, Loss: 0.0006028704228689811\n",
            "Epoch 107, Loss: 0.0005555230848877087\n",
            "Epoch 108, Loss: 0.0005118993235497868\n",
            "Epoch 109, Loss: 0.0004717039554910781\n",
            "Epoch 110, Loss: 0.0004346658492785467\n",
            "Epoch 111, Loss: 0.00040053580386471734\n",
            "Epoch 112, Loss: 0.00036908466145011635\n",
            "Epoch 113, Loss: 0.00034010159895038895\n",
            "Epoch 114, Loss: 0.0003133925931151976\n",
            "Epoch 115, Loss: 0.000288779025000321\n",
            "Epoch 116, Loss: 0.00026609641688942116\n",
            "Epoch 117, Loss: 0.00024519327981705277\n",
            "Epoch 118, Loss: 0.00022593006471222448\n",
            "Epoch 119, Loss: 0.000208178202671404\n",
            "Epoch 120, Loss: 0.00019181922802226562\n",
            "Epoch 121, Loss: 0.00017674397414874419\n",
            "Epoch 122, Loss: 0.00016285183658183038\n",
            "Epoch 123, Loss: 0.00015005009611836957\n",
            "Epoch 124, Loss: 0.0001382532973017763\n",
            "Epoch 125, Loss: 0.00012738267683816702\n",
            "Epoch 126, Loss: 0.00011736563801979086\n",
            "Epoch 127, Loss: 0.00010813526695319662\n",
            "Epoch 128, Loss: 9.962988729296327e-05\n",
            "Epoch 129, Loss: 9.179265013994984e-05\n",
            "Epoch 130, Loss: 8.457115633198284e-05\n",
            "Epoch 131, Loss: 7.791710841673557e-05\n",
            "Epoch 132, Loss: 7.178598997338847e-05\n",
            "Epoch 133, Loss: 6.613677005194942e-05\n",
            "Epoch 134, Loss: 6.093163076143353e-05\n",
            "Epoch 135, Loss: 5.613571615067942e-05\n",
            "Epoch 136, Loss: 5.171690071733506e-05\n",
            "Epoch 137, Loss: 4.7645575988854955e-05\n",
            "Epoch 138, Loss: 4.389445376590871e-05\n",
            "Epoch 139, Loss: 4.043838471695378e-05\n",
            "Epoch 140, Loss: 3.725419112826184e-05\n",
            "Epoch 141, Loss: 3.432051270079018e-05\n",
            "Epoch 142, Loss: 3.1617664378592443e-05\n",
            "Epoch 143, Loss: 2.9127505269073476e-05\n",
            "Epoch 144, Loss: 2.683331779236395e-05\n",
            "Epoch 145, Loss: 2.4719696262192736e-05\n",
            "Epoch 146, Loss: 2.2772444164613663e-05\n",
            "Epoch 147, Loss: 2.0978479456991492e-05\n",
            "Epoch 148, Loss: 1.932574726315585e-05\n",
            "Epoch 149, Loss: 1.7803139388644848e-05\n",
            "Epoch 150, Loss: 1.6400420125207076e-05\n",
            "Epoch 151, Loss: 1.5108157854482969e-05\n",
            "Epoch 152, Loss: 1.3917661999341183e-05\n",
            "Epoch 153, Loss: 1.2820924905968961e-05\n",
            "Epoch 154, Loss: 1.1810568272538691e-05\n",
            "Epoch 155, Loss: 1.0879793769883316e-05\n",
            "Epoch 156, Loss: 1.0022337527243283e-05\n",
            "Epoch 157, Loss: 9.2324281815257e-06\n",
            "Epoch 158, Loss: 8.504748211978948e-06\n",
            "Epoch 159, Loss: 7.834398303775124e-06\n",
            "Epoch 160, Loss: 7.216864503919053e-06\n",
            "Epoch 161, Loss: 6.647987951357968e-06\n",
            "Epoch 162, Loss: 6.123936980115231e-06\n",
            "Epoch 163, Loss: 5.641181409912789e-06\n",
            "Epoch 164, Loss: 5.196468853221575e-06\n",
            "Epoch 165, Loss: 4.786802881013925e-06\n",
            "Epoch 166, Loss: 4.409422901762351e-06\n",
            "Epoch 167, Loss: 4.06178561955375e-06\n",
            "Epoch 168, Loss: 3.7415479477476756e-06\n",
            "Epoch 169, Loss: 3.446551264099651e-06\n",
            "Epoch 170, Loss: 3.174806902306558e-06\n",
            "Epoch 171, Loss: 2.9244827830811458e-06\n",
            "Epoch 172, Loss: 2.6938910954251933e-06\n",
            "Epoch 173, Loss: 2.481476945752673e-06\n",
            "Epoch 174, Loss: 2.2858078990018573e-06\n",
            "Epoch 175, Loss: 2.1055643417289836e-06\n",
            "Epoch 176, Loss: 1.939530602721642e-06\n",
            "Epoch 177, Loss: 1.786586771660493e-06\n",
            "Epoch 178, Loss: 1.645701161082228e-06\n",
            "Epoch 179, Loss: 1.5159233610830244e-06\n",
            "Epoch 180, Loss: 1.3963778402789933e-06\n",
            "Epoch 181, Loss: 1.2862580500748751e-06\n",
            "Epoch 182, Loss: 1.184820992740895e-06\n",
            "Epoch 183, Loss: 1.0913822168460361e-06\n",
            "Epoch 184, Loss: 1.0053112064616776e-06\n",
            "Epoch 185, Loss: 9.26027133201267e-07\n",
            "Epoch 186, Loss: 8.529949425772512e-07\n",
            "Epoch 187, Loss: 7.857217483818055e-07\n",
            "Epoch 188, Loss: 7.237535108919607e-07\n",
            "Epoch 189, Loss: 6.66671976560727e-07\n",
            "Epoch 190, Loss: 6.14091858649718e-07\n",
            "Epoch 191, Loss: 5.65658239838829e-07\n",
            "Epoch 192, Loss: 5.210441793570132e-07\n",
            "Epoch 193, Loss: 4.799485085468589e-07\n",
            "Epoch 194, Loss: 4.4209380002710276e-07\n",
            "Epoch 195, Loss: 4.072244968075085e-07\n",
            "Epoch 196, Loss: 3.7510518875381823e-07\n",
            "Epoch 197, Loss: 3.455190248178321e-07\n",
            "Epoch 198, Loss: 3.1826625033544046e-07\n",
            "Epoch 199, Loss: 2.931628595586526e-07\n",
            "Epoch 200, Loss: 2.7003935433762886e-07\n",
            "Epoch 201, Loss: 2.487396006082452e-07\n",
            "Epoch 202, Loss: 2.29119774971925e-07\n",
            "Epoch 203, Loss: 2.1104739428320875e-07\n",
            "Epoch 204, Loss: 1.9440042170001464e-07\n",
            "Epoch 205, Loss: 1.7906644318519734e-07\n",
            "Epoch 206, Loss: 1.6494190889565706e-07\n",
            "Epoch 207, Loss: 1.5193143436275136e-07\n",
            "Epoch 208, Loss: 1.3994715674795898e-07\n",
            "Epoch 209, Loss: 1.289081418317784e-07\n",
            "Epoch 210, Loss: 1.1873983774286816e-07\n",
            "Epoch 211, Loss: 1.093735717505312e-07\n",
            "Epoch 212, Loss: 1.0074608671234146e-07\n",
            "Epoch 213, Loss: 9.279911407050077e-08\n",
            "Epoch 214, Loss: 8.547898050307655e-08\n",
            "Epoch 215, Loss: 7.873624558479247e-08\n",
            "Epoch 216, Loss: 7.252536801325745e-08\n",
            "Epoch 217, Loss: 6.680439814673878e-08\n",
            "Epoch 218, Loss: 6.153469478321875e-08\n",
            "Epoch 219, Loss: 5.6680664265716315e-08\n",
            "Epoch 220, Loss: 5.220952016195252e-08\n",
            "Epoch 221, Loss: 4.80910618861674e-08\n",
            "Epoch 222, Loss: 4.4297470779423334e-08\n",
            "Epoch 223, Loss: 4.0803122266836816e-08\n",
            "Epoch 224, Loss: 3.758441282656265e-08\n",
            "Epoch 225, Loss: 3.461960060127108e-08\n",
            "Epoch 226, Loss: 3.188865857977254e-08\n",
            "Epoch 227, Loss: 2.9373139353436788e-08\n",
            "Epoch 228, Loss: 2.7056050539092574e-08\n",
            "Epoch 229, Loss: 2.492174002421556e-08\n",
            "Epoch 230, Loss: 2.2955790262914762e-08\n",
            "Epoch 231, Loss: 2.1144920905727287e-08\n",
            "Epoch 232, Loss: 1.9476899109756667e-08\n",
            "Epoch 233, Loss: 1.7940456920804988e-08\n",
            "Epoch 234, Loss: 1.6525215171337096e-08\n",
            "Epoch 235, Loss: 1.5221613379883902e-08\n",
            "Epoch 236, Loss: 1.402084517928977e-08\n",
            "Epoch 237, Loss: 1.2914798836891895e-08\n",
            "Epoch 238, Loss: 1.1896002465970634e-08\n",
            "Epoch 239, Loss: 1.0957573558550033e-08\n",
            "Epoch 240, Loss: 1.0093172497174512e-08\n",
            "Epoch 241, Loss: 9.296959735138903e-09\n",
            "Epoch 242, Loss: 8.563556351975196e-09\n",
            "Epoch 243, Loss: 7.888007720814931e-09\n",
            "Epoch 244, Loss: 7.265750042045761e-09\n",
            "Epoch 245, Loss: 6.692579514808337e-09\n",
            "Epoch 246, Loss: 6.164623941694111e-09\n",
            "Epoch 247, Loss: 5.678316571817741e-09\n",
            "Epoch 248, Loss: 5.2303720079418486e-09\n",
            "Epoch 249, Loss: 4.8177640135047556e-09\n",
            "Epoch 250, Loss: 4.437705069466227e-09\n",
            "Epoch 251, Loss: 4.0876275451767995e-09\n",
            "Epoch 252, Loss: 3.765166352849905e-09\n",
            "Epoch 253, Loss: 3.468142970996409e-09\n",
            "Epoch 254, Loss: 3.1945507280315455e-09\n",
            "Epoch 255, Loss: 2.942541246253458e-09\n",
            "Epoch 256, Loss: 2.7104119555919574e-09\n",
            "Epoch 257, Loss: 2.496594592060453e-09\n",
            "Epoch 258, Loss: 2.2996446029394557e-09\n",
            "Epoch 259, Loss: 2.11823138888962e-09\n",
            "Epoch 260, Loss: 1.951129314205735e-09\n",
            "Epoch 261, Loss: 1.797209427492772e-09\n",
            "Epoch 262, Loss: 1.6554318348259965e-09\n",
            "Epoch 263, Loss: 1.5248386743877175e-09\n",
            "Epoch 264, Loss: 1.4045476458327124e-09\n",
            "Epoch 265, Loss: 1.2937460491985515e-09\n",
            "Epoch 266, Loss: 1.1916852950533247e-09\n",
            "Epoch 267, Loss: 1.0976758469913712e-09\n",
            "Epoch 268, Loss: 1.0110825631136063e-09\n",
            "Epoch 269, Loss: 9.313204054236322e-10\n",
            "Epoch 270, Loss: 8.578504872144327e-10\n",
            "Epoch 271, Loss: 7.901764325041301e-10\n",
            "Epoch 272, Loss: 7.278410225873914e-10\n",
            "Epoch 273, Loss: 6.70423106962445e-10\n",
            "Epoch 274, Loss: 6.175347583011128e-10\n",
            "Epoch 275, Loss: 5.688186514711848e-10\n",
            "Epoch 276, Loss: 5.239456495441636e-10\n",
            "Epoch 277, Loss: 4.826125801075815e-10\n",
            "Epoch 278, Loss: 4.4454018700688936e-10\n",
            "Epoch 279, Loss: 4.0947124373830275e-10\n",
            "Epoch 280, Loss: 3.7716881566693055e-10\n",
            "Epoch 281, Loss: 3.474146591154691e-10\n",
            "Epoch 282, Loss: 3.200077470977861e-10\n",
            "Epoch 283, Loss: 2.9476291102889296e-10\n",
            "Epoch 284, Loss: 2.7150958976485195e-10\n",
            "Epoch 285, Loss: 2.500906772004238e-10\n",
            "Epoch 286, Loss: 2.303614609984612e-10\n",
            "Epoch 287, Loss: 2.121886447374774e-10\n",
            "Epoch 288, Loss: 1.9544944741647924e-10\n",
            "Epoch 289, Loss: 1.8003077390508957e-10\n",
            "Epoch 290, Loss: 1.6582845087023214e-10\n",
            "Epoch 291, Loss: 1.5274652297945253e-10\n",
            "Epoch 292, Loss: 1.4069660455032145e-10\n",
            "Epoch 293, Loss: 1.2959728245575684e-10\n",
            "Epoch 294, Loss: 1.1937356604760967e-10\n",
            "Epoch 295, Loss: 1.0995638052219522e-10\n",
            "Epoch 296, Loss: 1.012821002423365e-10\n",
            "Epoch 297, Loss: 9.329211885515509e-11\n",
            "Epoch 298, Loss: 8.593245331652431e-11\n",
            "Epoch 299, Loss: 7.915337919610524e-11\n",
            "Epoch 300, Loss: 7.290909471855501e-11\n",
            "Epoch 301, Loss: 6.715741133989387e-11\n",
            "Epoch 302, Loss: 6.185946863470225e-11\n",
            "Epoch 303, Loss: 5.697947182839177e-11\n",
            "Epoch 304, Loss: 5.248444992923446e-11\n",
            "Epoch 305, Loss: 4.834403294199733e-11\n",
            "Epoch 306, Loss: 4.453024670844076e-11\n",
            "Epoch 307, Loss: 4.101732390008057e-11\n",
            "Epoch 308, Loss: 3.7781529922078167e-11\n",
            "Epoch 309, Loss: 3.4801002554321544e-11\n",
            "Epoch 310, Loss: 3.2055604258178933e-11\n",
            "Epoch 311, Loss: 2.952678608971959e-11\n",
            "Epoch 312, Loss: 2.719746240163352e-11\n",
            "Epoch 313, Loss: 2.5051895410474616e-11\n",
            "Epoch 314, Loss: 2.3075588862886935e-11\n",
            "Epoch 315, Loss: 2.1255190074312906e-11\n",
            "Epoch 316, Loss: 1.957839972917313e-11\n",
            "Epoch 317, Loss: 1.8033888797269797e-11\n",
            "Epoch 318, Loss: 1.6611221973360127e-11\n",
            "Epoch 319, Loss: 1.5300787158125772e-11\n",
            "Epoch 320, Loss: 1.4093730558214566e-11\n",
            "Epoch 321, Loss: 1.2981896827341786e-11\n",
            "Epoch 322, Loss: 1.1957773982220298e-11\n",
            "Epoch 323, Loss: 1.1014442651774952e-11\n",
            "Epoch 324, Loss: 1.0145529314638308e-11\n",
            "Epoch 325, Loss: 9.345163253271147e-12\n",
            "Epoch 326, Loss: 8.607936877732005e-12\n",
            "Epoch 327, Loss: 7.928869200256564e-12\n",
            "Epoch 328, Loss: 7.303372165256682e-12\n",
            "Epoch 329, Loss: 6.727219669532485e-12\n",
            "Epoch 330, Loss: 6.196518995318768e-12\n",
            "Epoch 331, Loss: 5.7076845143539334e-12\n",
            "Epoch 332, Loss: 5.257413474426674e-12\n",
            "Epoch 333, Loss: 4.842663657188753e-12\n",
            "Epoch 334, Loss: 4.4606328490978426e-12\n",
            "Epoch 335, Loss: 4.108739894918233e-12\n",
            "Epoch 336, Loss: 3.784607266667834e-12\n",
            "Epoch 337, Loss: 3.486044990634193e-12\n",
            "Epoch 338, Loss: 3.2110358596594513e-12\n",
            "Epoch 339, Loss: 2.957721803417808e-12\n",
            "Epoch 340, Loss: 2.7243913283409056e-12\n",
            "Epoch 341, Loss: 2.509467960100932e-12\n",
            "Epoch 342, Loss: 2.3114995851557723e-12\n",
            "Epoch 343, Loss: 2.1291486526947877e-12\n",
            "Epoch 344, Loss: 1.961183124636227e-12\n",
            "Epoch 345, Loss: 1.8064681565108656e-12\n",
            "Epoch 346, Loss: 1.6639584316608618e-12\n",
            "Epoch 347, Loss: 1.5326910950301295e-12\n",
            "Epoch 348, Loss: 1.4117792535935503e-12\n",
            "Epoch 349, Loss: 1.3004059742198445e-12\n",
            "Epoch 350, Loss: 1.1978187747347383e-12\n",
            "Epoch 351, Loss: 1.1033245340641452e-12\n",
            "Epoch 352, Loss: 1.0162848099574218e-12\n",
            "Epoch 353, Loss: 9.36111527162979e-13\n",
            "Epoch 354, Loss: 8.622630007192629e-13\n",
            "Epoch 355, Loss: 7.942402808674591e-13\n",
            "Epoch 356, Loss: 7.315837763744011e-13\n",
            "Epoch 357, Loss: 6.73870156259524e-13\n",
            "Epoch 358, Loss: 6.207094821848633e-13\n",
            "Epoch 359, Loss: 5.717425777169289e-13\n",
            "Epoch 360, Loss: 5.266386047257726e-13\n",
            "Epoch 361, Loss: 4.850928207246058e-13\n",
            "Epoch 362, Loss: 4.468245251131114e-13\n",
            "Epoch 363, Loss: 4.1157516121123063e-13\n",
            "Epoch 364, Loss: 3.791065707245459e-13\n",
            "Epoch 365, Loss: 3.491993814680684e-13\n",
            "Epoch 366, Loss: 3.2165152845137775e-13\n",
            "Epoch 367, Loss: 2.962768875142749e-13\n",
            "Epoch 368, Loss: 2.7290401611110853e-13\n",
            "Epoch 369, Loss: 2.5137499795573235e-13\n",
            "Epoch 370, Loss: 2.315443737432975e-13\n",
            "Epoch 371, Loss: 2.1327816023715484e-13\n",
            "Epoch 372, Loss: 1.9645294245137275e-13\n",
            "Epoch 373, Loss: 1.8095504276625416e-13\n",
            "Epoch 374, Loss: 1.6667975097657033e-13\n",
            "Epoch 375, Loss: 1.5353061701605324e-13\n",
            "Epoch 376, Loss: 1.414187995702032e-13\n",
            "Epoch 377, Loss: 1.3026246680169949e-13\n",
            "Epoch 378, Loss: 1.199862416112439e-13\n",
            "Epoch 379, Loss: 1.1052069316607318e-13\n",
            "Epoch 380, Loss: 1.0180186919996503e-13\n",
            "Epoch 381, Loss: 9.377086040555562e-14\n",
            "Epoch 382, Loss: 8.637340769254716e-14\n",
            "Epoch 383, Loss: 7.955952910242035e-14\n",
            "Epoch 384, Loss: 7.32831881111617e-14\n",
            "Epoch 385, Loss: 6.750197915785192e-14\n",
            "Epoch 386, Loss: 6.21768413134987e-14\n",
            "Epoch 387, Loss: 5.727179629042625e-14\n",
            "Epoch 388, Loss: 5.275370382401914e-14\n",
            "Epoch 389, Loss: 4.8592037140227325e-14\n",
            "Epoch 390, Loss: 4.4758678469698424e-14\n",
            "Epoch 391, Loss: 4.122772830662311e-14\n",
            "Epoch 392, Loss: 3.797532987004332e-14\n",
            "Epoch 393, Loss: 3.4979508638855135e-14\n",
            "Epoch 394, Loss: 3.2220023594743526e-14\n",
            "Epoch 395, Loss: 2.967823046104229e-14\n",
            "Epoch 396, Loss: 2.7336955919937938e-14\n",
            "Epoch 397, Loss: 2.5180381334356584e-14\n",
            "Epoch 398, Loss: 2.319393586790087e-14\n",
            "Epoch 399, Loss: 2.1364198317046613e-14\n",
            "Epoch 400, Loss: 1.9678806256483048e-14\n",
            "Epoch 401, Loss: 1.8126372387118467e-14\n",
            "Epoch 402, Loss: 1.66964080072947e-14\n",
            "Epoch 403, Loss: 1.5379251423100563e-14\n",
            "Epoch 404, Loss: 1.4166003560187258e-14\n",
            "Epoch 405, Loss: 1.30484670947012e-14\n",
            "Epoch 406, Loss: 1.2019091568816805e-14\n",
            "Epoch 407, Loss: 1.1070922039497425e-14\n",
            "Epoch 408, Loss: 1.0197552270974853e-14\n",
            "Epoch 409, Loss: 9.393081475803816e-15\n",
            "Epoch 410, Loss: 8.652074170231285e-15\n",
            "Epoch 411, Loss: 7.969524014436696e-15\n",
            "Epoch 412, Loss: 7.340819366928655e-15\n",
            "Epoch 413, Loss: 6.761712170684102e-15\n",
            "Epoch 414, Loss: 6.22829006610456e-15\n",
            "Epoch 415, Loss: 5.7369488493954735e-15\n",
            "Epoch 416, Loss: 5.284368865170546e-15\n",
            "Epoch 417, Loss: 4.867492322205835e-15\n",
            "Epoch 418, Loss: 4.48350254782253e-15\n",
            "Epoch 419, Loss: 4.129805296272884e-15\n",
            "Epoch 420, Loss: 3.804010586404018e-15\n",
            "Epoch 421, Loss: 3.503917484421167e-15\n",
            "Epoch 422, Loss: 3.2274982636637785e-15\n",
            "Epoch 423, Loss: 2.972885375209342e-15\n",
            "Epoch 424, Loss: 2.7383585506811254e-15\n",
            "Epoch 425, Loss: 2.522333190956487e-15\n",
            "Epoch 426, Loss: 2.323349857268115e-15\n",
            "Epoch 427, Loss: 2.1400639619298973e-15\n",
            "Epoch 428, Loss: 1.971237272930804e-15\n",
            "Epoch 429, Loss: 1.8157291045831353e-15\n",
            "Epoch 430, Loss: 1.672488705197188e-15\n",
            "Epoch 431, Loss: 1.5405483862435653e-15\n",
            "Epoch 432, Loss: 1.4190166575895116e-15\n",
            "Epoch 433, Loss: 1.3070724036633398e-15\n",
            "Epoch 434, Loss: 1.203959286709403e-15\n",
            "Epoch 435, Loss: 1.108980547894371e-15\n",
            "Epoch 436, Loss: 1.0214946265346689e-15\n",
            "Epoch 437, Loss: 9.4091032678579e-16\n",
            "Epoch 438, Loss: 8.666832090383777e-16\n",
            "Epoch 439, Loss: 7.983117820045052e-16\n",
            "Epoch 440, Loss: 7.353340456044801e-16\n",
            "Epoch 441, Loss: 6.773245525268105e-16\n",
            "Epoch 442, Loss: 6.238913692237342e-16\n",
            "Epoch 443, Loss: 5.746734451134418e-16\n",
            "Epoch 444, Loss: 5.293382331253085e-16\n",
            "Epoch 445, Loss: 4.875794612349375e-16\n",
            "Epoch 446, Loss: 4.491149892901132e-16\n",
            "Epoch 447, Loss: 4.13684935302192e-16\n",
            "Epoch 448, Loss: 3.810498957648699e-16\n",
            "Epoch 449, Loss: 3.5098939456554755e-16\n",
            "Epoch 450, Loss: 3.2330034324375417e-16\n",
            "Epoch 451, Loss: 2.9779560717764666e-16\n",
            "Epoch 452, Loss: 2.743029257325823e-16\n",
            "Epoch 453, Loss: 2.5266354012817397e-16\n",
            "Epoch 454, Loss: 2.3273126459280973e-16\n",
            "Epoch 455, Loss: 2.143714111838986e-16\n",
            "Epoch 456, Loss: 1.9745995444893453e-16\n",
            "Epoch 457, Loss: 1.8188261403047642e-16\n",
            "Epoch 458, Loss: 1.6753413740869749e-16\n",
            "Epoch 459, Loss: 1.543175971343688e-16\n",
            "Epoch 460, Loss: 1.4214369855691974e-16\n",
            "Epoch 461, Loss: 1.309301751589151e-16\n",
            "Epoch 462, Loss: 1.2060127757314978e-16\n",
            "Epoch 463, Loss: 1.1108720335011507e-16\n",
            "Epoch 464, Loss: 1.0232369752289376e-16\n",
            "Epoch 465, Loss: 9.42515162877143e-17\n",
            "Epoch 466, Loss: 8.681613956829559e-17\n",
            "Epoch 467, Loss: 7.996734331869408e-17\n",
            "Epoch 468, Loss: 7.365882620059381e-17\n",
            "Epoch 469, Loss: 6.78479818229011e-17\n",
            "Epoch 470, Loss: 6.24955467659095e-17\n",
            "Epoch 471, Loss: 5.75653544123934e-17\n",
            "Epoch 472, Loss: 5.302410911604191e-17\n",
            "Epoch 473, Loss: 4.88411072891239e-17\n",
            "Epoch 474, Loss: 4.498810040028749e-17\n",
            "Epoch 475, Loss: 4.143904907909957e-17\n",
            "Epoch 476, Loss: 3.8169978269658654e-17\n",
            "Epoch 477, Loss: 3.5158804391184795e-17\n",
            "Epoch 478, Loss: 3.238517626779711e-17\n",
            "Epoch 479, Loss: 2.9830350941878444e-17\n",
            "Epoch 480, Loss: 2.747707952188288e-17\n",
            "Epoch 481, Loss: 2.5309449105825793e-17\n",
            "Epoch 482, Loss: 2.3312818835698534e-17\n",
            "Epoch 483, Loss: 2.1473702120269877e-17\n",
            "Epoch 484, Loss: 1.9779672476667487e-17\n",
            "Epoch 485, Loss: 1.8219279588546543e-17\n",
            "Epoch 486, Loss: 1.6781989929895457e-17\n",
            "Epoch 487, Loss: 1.5458081906459252e-17\n",
            "Epoch 488, Loss: 1.4238611531564045e-17\n",
            "Epoch 489, Loss: 1.3115346120181109e-17\n",
            "Epoch 490, Loss: 1.2080696652535297e-17\n",
            "Epoch 491, Loss: 1.1127667484394873e-17\n",
            "Epoch 492, Loss: 1.0249822601355476e-17\n",
            "Epoch 493, Loss: 9.441226142525689e-18\n",
            "Epoch 494, Loss: 8.696421499396849e-18\n",
            "Epoch 495, Loss: 8.010369269947579e-18\n",
            "Epoch 496, Loss: 7.378446664373933e-18\n",
            "Epoch 497, Loss: 6.796367855893668e-18\n",
            "Epoch 498, Loss: 6.260213221065066e-18\n",
            "Epoch 499, Loss: 5.7663522919364784e-18\n",
            "Epoch 500, Loss: 5.3114541517748744e-18\n",
            "Epoch 501, Loss: 4.892439891590811e-18\n",
            "Epoch 502, Loss: 4.506480839309981e-18\n",
            "Epoch 503, Loss: 4.1509720717272305e-18\n",
            "Epoch 504, Loss: 3.823507903560769e-18\n",
            "Epoch 505, Loss: 3.521877435362747e-18\n",
            "Epoch 506, Loss: 3.244040707151683e-18\n",
            "Epoch 507, Loss: 2.9881226023107413e-18\n",
            "Epoch 508, Loss: 2.7523939367951954e-18\n",
            "Epoch 509, Loss: 2.5352624768140003e-18\n",
            "Epoch 510, Loss: 2.33525845957718e-18\n",
            "Epoch 511, Loss: 2.1510327008292145e-18\n",
            "Epoch 512, Loss: 1.9813408249187536e-18\n",
            "Epoch 513, Loss: 1.8250350628227537e-18\n",
            "Epoch 514, Loss: 1.6810609586925668e-18\n",
            "Epoch 515, Loss: 1.5484445918042988e-18\n",
            "Epoch 516, Loss: 1.4262899006110526e-18\n",
            "Epoch 517, Loss: 1.3137719937758178e-18\n",
            "Epoch 518, Loss: 1.2101303574635926e-18\n",
            "Epoch 519, Loss: 1.1146638985426004e-18\n",
            "Epoch 520, Loss: 1.0267300766695871e-18\n",
            "Epoch 521, Loss: 9.457324589463471e-19\n",
            "Epoch 522, Loss: 8.711255581767236e-19\n",
            "Epoch 523, Loss: 8.024036025158907e-19\n",
            "Epoch 524, Loss: 7.391026141143156e-19\n",
            "Epoch 525, Loss: 6.807960091925398e-19\n",
            "Epoch 526, Loss: 6.270890476737575e-19\n",
            "Epoch 527, Loss: 5.776188816762174e-19\n",
            "Epoch 528, Loss: 5.320515366572803e-19\n",
            "Epoch 529, Loss: 4.900785872489181e-19\n",
            "Epoch 530, Loss: 4.514168025571195e-19\n",
            "Epoch 531, Loss: 4.1580500330911704e-19\n",
            "Epoch 532, Loss: 3.830034336167039e-19\n",
            "Epoch 533, Loss: 3.5278822073922783e-19\n",
            "Epoch 534, Loss: 3.249572068529083e-19\n",
            "Epoch 535, Loss: 2.9932174459854246e-19\n",
            "Epoch 536, Loss: 2.7570926034001407e-19\n",
            "Epoch 537, Loss: 2.5395861527368163e-19\n",
            "Epoch 538, Loss: 2.339242542028653e-19\n",
            "Epoch 539, Loss: 2.1547012018386731e-19\n",
            "Epoch 540, Loss: 1.984721212714413e-19\n",
            "Epoch 541, Loss: 1.8281466892383788e-19\n",
            "Epoch 542, Loss: 1.683926146240699e-19\n",
            "Epoch 543, Loss: 1.5510847725876688e-19\n",
            "Epoch 544, Loss: 1.428723303942411e-19\n",
            "Epoch 545, Loss: 1.3160123000502296e-19\n",
            "Epoch 546, Loss: 1.2121971967560726e-19\n",
            "Epoch 547, Loss: 1.116567040435751e-19\n",
            "Epoch 548, Loss: 1.028482862694323e-19\n",
            "Epoch 549, Loss: 9.473487653064947e-20\n",
            "Epoch 550, Loss: 8.72611718991885e-20\n",
            "Epoch 551, Loss: 8.037722428250038e-20\n",
            "Epoch 552, Loss: 7.403622369498416e-20\n",
            "Epoch 553, Loss: 6.819587245003528e-20\n",
            "Epoch 554, Loss: 6.281572249682886e-20\n",
            "Epoch 555, Loss: 5.786049148805093e-20\n",
            "Epoch 556, Loss: 5.329602891447772e-20\n",
            "Epoch 557, Loss: 4.909159033511648e-20\n",
            "Epoch 558, Loss: 4.5218745877714946e-20\n",
            "Epoch 559, Loss: 4.1651439705955976e-20\n",
            "Epoch 560, Loss: 3.8365723327427084e-20\n",
            "Epoch 561, Loss: 3.5338894737312376e-20\n",
            "Epoch 562, Loss: 3.255116666816717e-20\n",
            "Epoch 563, Loss: 2.9983259477106755e-20\n",
            "Epoch 564, Loss: 2.7617992545786976e-20\n",
            "Epoch 565, Loss: 2.5439223273037753e-20\n",
            "Epoch 566, Loss: 2.3432390618306152e-20\n",
            "Epoch 567, Loss: 2.158378028156887e-20\n",
            "Epoch 568, Loss: 1.98810977815797e-20\n",
            "Epoch 569, Loss: 1.8312578433956314e-20\n",
            "Epoch 570, Loss: 1.6867928373937083e-20\n",
            "Epoch 571, Loss: 1.5537324439375922e-20\n",
            "Epoch 572, Loss: 1.4311605294569573e-20\n",
            "Epoch 573, Loss: 1.3182732022918076e-20\n",
            "Epoch 574, Loss: 1.2142545326611954e-20\n",
            "Epoch 575, Loss: 1.1184732812424666e-20\n",
            "Epoch 576, Loss: 1.0302391836736157e-20\n",
            "Epoch 577, Loss: 9.489662409186672e-21\n",
            "Epoch 578, Loss: 8.740975667226977e-21\n",
            "Epoch 579, Loss: 8.05139972786315e-21\n",
            "Epoch 580, Loss: 7.4163108705372e-21\n",
            "Epoch 581, Loss: 6.831218482544334e-21\n",
            "Epoch 582, Loss: 6.292335446673085e-21\n",
            "Epoch 583, Loss: 5.795871077176421e-21\n",
            "Epoch 584, Loss: 5.338651932682841e-21\n",
            "Epoch 585, Loss: 4.91755584765642e-21\n",
            "Epoch 586, Loss: 4.529576600674679e-21\n",
            "Epoch 587, Loss: 4.1722568479370564e-21\n",
            "Epoch 588, Loss: 3.843102048583333e-21\n",
            "Epoch 589, Loss: 3.539931007670969e-21\n",
            "Epoch 590, Loss: 3.2606487698134834e-21\n",
            "Epoch 591, Loss: 3.0034490611525957e-21\n",
            "Epoch 592, Loss: 2.7665209759469396e-21\n",
            "Epoch 593, Loss: 2.5482448238765797e-21\n",
            "Epoch 594, Loss: 2.34722420923196e-21\n",
            "Epoch 595, Loss: 2.162063905518645e-21\n",
            "Epoch 596, Loss: 1.9914990478240663e-21\n",
            "Epoch 597, Loss: 1.8344044410064256e-21\n",
            "Epoch 598, Loss: 1.6896874884175408e-21\n",
            "Epoch 599, Loss: 1.5564008363467099e-21\n",
            "Epoch 600, Loss: 1.4336158479298686e-21\n",
            "Epoch 601, Loss: 1.320490398496735e-21\n",
            "Epoch 602, Loss: 1.2163162729964005e-21\n",
            "Epoch 603, Loss: 1.1203537906023176e-21\n",
            "Epoch 604, Loss: 1.0320063400273433e-21\n",
            "Epoch 605, Loss: 9.505818246456668e-22\n",
            "Epoch 606, Loss: 8.75591786887719e-22\n",
            "Epoch 607, Loss: 8.065239170253637e-22\n",
            "Epoch 608, Loss: 7.428899109594558e-22\n",
            "Epoch 609, Loss: 6.842795788426064e-22\n",
            "Epoch 610, Loss: 6.302877933669379e-22\n",
            "Epoch 611, Loss: 5.805579408143889e-22\n",
            "Epoch 612, Loss: 5.34769396447708e-22\n",
            "Epoch 613, Loss: 4.925865426857979e-22\n",
            "Epoch 614, Loss: 4.537136323582178e-22\n",
            "Epoch 615, Loss: 4.179285764435929e-22\n",
            "Epoch 616, Loss: 3.849705620575878e-22\n",
            "Epoch 617, Loss: 3.5458557778831024e-22\n",
            "Epoch 618, Loss: 3.266255058796008e-22\n",
            "Epoch 619, Loss: 3.008469848869143e-22\n",
            "Epoch 620, Loss: 2.771168533761423e-22\n",
            "Epoch 621, Loss: 2.5526048213314225e-22\n",
            "Epoch 622, Loss: 2.3512187937430336e-22\n",
            "Epoch 623, Loss: 2.1656037836890903e-22\n",
            "Epoch 624, Loss: 1.994744303222032e-22\n",
            "Epoch 625, Loss: 1.8374491186338255e-22\n",
            "Epoch 626, Loss: 1.6925076310370697e-22\n",
            "Epoch 627, Loss: 1.5590007814854031e-22\n",
            "Epoch 628, Loss: 1.4359843366320863e-22\n",
            "Epoch 629, Loss: 1.3228497415647e-22\n",
            "Epoch 630, Loss: 1.2184002012290889e-22\n",
            "Epoch 631, Loss: 1.122197478863759e-22\n",
            "Epoch 632, Loss: 1.0338017036558071e-22\n",
            "Epoch 633, Loss: 9.52239982325509e-23\n",
            "Epoch 634, Loss: 8.770768261822897e-23\n",
            "Epoch 635, Loss: 8.078366661153538e-23\n",
            "Epoch 636, Loss: 7.441612599265606e-23\n",
            "Epoch 637, Loss: 6.854389617245972e-23\n",
            "Epoch 638, Loss: 6.313419352153576e-23\n",
            "Epoch 639, Loss: 5.815930893080975e-23\n",
            "Epoch 640, Loss: 5.356641864704246e-23\n",
            "Epoch 641, Loss: 4.934552485666232e-23\n",
            "Epoch 642, Loss: 4.545112703640634e-23\n",
            "Epoch 643, Loss: 4.186865917780841e-23\n",
            "Epoch 644, Loss: 3.856062368985928e-23\n",
            "Epoch 645, Loss: 3.552397630902189e-23\n",
            "Epoch 646, Loss: 3.272051629804949e-23\n",
            "Epoch 647, Loss: 3.0137516902328396e-23\n",
            "Epoch 648, Loss: 2.775577992215703e-23\n",
            "Epoch 649, Loss: 2.5567615492127498e-23\n",
            "Epoch 650, Loss: 2.354796394181935e-23\n",
            "Epoch 651, Loss: 2.1695724317231734e-23\n",
            "Epoch 652, Loss: 1.9982546498174967e-23\n",
            "Epoch 653, Loss: 1.8406645403354096e-23\n",
            "Epoch 654, Loss: 1.695516574313169e-23\n",
            "Epoch 655, Loss: 1.5619992160249093e-23\n",
            "Epoch 656, Loss: 1.43816893036951e-23\n",
            "Epoch 657, Loss: 1.3250061272385266e-23\n",
            "Epoch 658, Loss: 1.2206516291112957e-23\n",
            "Epoch 659, Loss: 1.1243316534192422e-23\n",
            "Epoch 660, Loss: 1.0355501853792812e-23\n",
            "Epoch 661, Loss: 9.537593463015945e-24\n",
            "Epoch 662, Loss: 8.78362688316506e-24\n",
            "Epoch 663, Loss: 8.091743410036035e-24\n",
            "Epoch 664, Loss: 7.456330446199644e-24\n",
            "Epoch 665, Loss: 6.865984304691672e-24\n",
            "Epoch 666, Loss: 6.323489344032807e-24\n",
            "Epoch 667, Loss: 5.82571895100306e-24\n",
            "Epoch 668, Loss: 5.367111668546575e-24\n",
            "Epoch 669, Loss: 4.942096129542374e-24\n",
            "Epoch 670, Loss: 4.553011959849874e-24\n",
            "Epoch 671, Loss: 4.19386324959562e-24\n",
            "Epoch 672, Loss: 3.8617294384932566e-24\n",
            "Epoch 673, Loss: 3.556408652106758e-24\n",
            "Epoch 674, Loss: 3.2773549370599527e-24\n",
            "Epoch 675, Loss: 3.0197168973933447e-24\n",
            "Epoch 676, Loss: 2.7812929086021236e-24\n",
            "Epoch 677, Loss: 2.5616116139694683e-24\n",
            "Epoch 678, Loss: 2.3594256395149497e-24\n",
            "Epoch 679, Loss: 2.173646135322233e-24\n",
            "Epoch 680, Loss: 2.0012052829606724e-24\n",
            "Epoch 681, Loss: 1.8434746157216073e-24\n",
            "Epoch 682, Loss: 1.6967897267888665e-24\n",
            "Epoch 683, Loss: 1.5637565811849928e-24\n",
            "Epoch 684, Loss: 1.4406612340941571e-24\n",
            "Epoch 685, Loss: 1.3274823616200154e-24\n",
            "Epoch 686, Loss: 1.2214282959525552e-24\n",
            "Epoch 687, Loss: 1.1264803071468621e-24\n",
            "Epoch 688, Loss: 1.037073154079925e-24\n",
            "Epoch 689, Loss: 9.554827374411614e-25\n",
            "Epoch 690, Loss: 8.80199017220607e-25\n",
            "Epoch 691, Loss: 8.102245527593194e-25\n",
            "Epoch 692, Loss: 7.464470221168505e-25\n",
            "Epoch 693, Loss: 6.878783770094109e-25\n",
            "Epoch 694, Loss: 6.332814123666778e-25\n",
            "Epoch 695, Loss: 5.840871095447706e-25\n",
            "Epoch 696, Loss: 5.379894555544051e-25\n",
            "Epoch 697, Loss: 4.946763449740018e-25\n",
            "Epoch 698, Loss: 4.5594572739202725e-25\n",
            "Epoch 699, Loss: 4.200490186563494e-25\n",
            "Epoch 700, Loss: 3.871060393429002e-25\n",
            "Epoch 701, Loss: 3.563435578531434e-25\n",
            "Epoch 702, Loss: 3.2817772296648634e-25\n",
            "Epoch 703, Loss: 3.019647625545105e-25\n",
            "Epoch 704, Loss: 2.7862233139116582e-25\n",
            "Epoch 705, Loss: 2.5663745588999665e-25\n",
            "Epoch 706, Loss: 2.3651795293278735e-25\n",
            "Epoch 707, Loss: 2.178470697685e-25\n",
            "Epoch 708, Loss: 2.003773505919281e-25\n",
            "Epoch 709, Loss: 1.848000840750781e-25\n",
            "Epoch 710, Loss: 1.7001278779909707e-25\n",
            "Epoch 711, Loss: 1.568015123522312e-25\n",
            "Epoch 712, Loss: 1.4470683253885073e-25\n",
            "Epoch 713, Loss: 1.3305071008496656e-25\n",
            "Epoch 714, Loss: 1.224731207258909e-25\n",
            "Epoch 715, Loss: 1.128810774824208e-25\n",
            "Epoch 716, Loss: 1.0384076848644294e-25\n",
            "Epoch 717, Loss: 9.57317714188367e-26\n",
            "Epoch 718, Loss: 8.80841593731716e-26\n",
            "Epoch 719, Loss: 8.138314343591678e-26\n",
            "Epoch 720, Loss: 7.462205081034873e-26\n",
            "Epoch 721, Loss: 6.884477866715607e-26\n",
            "Epoch 722, Loss: 6.35918771588058e-26\n",
            "Epoch 723, Loss: 5.869068435466765e-26\n",
            "Epoch 724, Loss: 5.391786633690258e-26\n",
            "Epoch 725, Loss: 4.9647133633407395e-26\n",
            "Epoch 726, Loss: 4.5732805821700734e-26\n",
            "Epoch 727, Loss: 4.210019996077113e-26\n",
            "Epoch 728, Loss: 3.8769814108202677e-26\n",
            "Epoch 729, Loss: 3.5770306101567865e-26\n",
            "Epoch 730, Loss: 3.303523906150511e-26\n",
            "Epoch 731, Loss: 3.047862714934532e-26\n",
            "Epoch 732, Loss: 2.7958524705955287e-26\n",
            "Epoch 733, Loss: 2.5862656866251143e-26\n",
            "Epoch 734, Loss: 2.370105472642913e-26\n",
            "Epoch 735, Loss: 2.1816342764339692e-26\n",
            "Epoch 736, Loss: 2.016786999146066e-26\n",
            "Epoch 737, Loss: 1.8460306606399914e-26\n",
            "Epoch 738, Loss: 1.7040246043437296e-26\n",
            "Epoch 739, Loss: 1.5717375609188236e-26\n",
            "Epoch 740, Loss: 1.4471357483440004e-26\n",
            "Epoch 741, Loss: 1.332824872796818e-26\n",
            "Epoch 742, Loss: 1.2287753519933311e-26\n",
            "Epoch 743, Loss: 1.1308222468730052e-26\n",
            "Epoch 744, Loss: 1.0478217536921106e-26\n",
            "Epoch 745, Loss: 9.624657711520328e-27\n",
            "Epoch 746, Loss: 8.876373839111622e-27\n",
            "Epoch 747, Loss: 8.149783641596493e-27\n",
            "Epoch 748, Loss: 7.515970882106343e-27\n",
            "Epoch 749, Loss: 6.946425634488416e-27\n",
            "Epoch 750, Loss: 6.3447219790310894e-27\n",
            "Epoch 751, Loss: 5.874351338341417e-27\n",
            "Epoch 752, Loss: 5.35449200180077e-27\n",
            "Epoch 753, Loss: 4.9887193867627464e-27\n",
            "Epoch 754, Loss: 4.553600967775138e-27\n",
            "Epoch 755, Loss: 4.282886091816246e-27\n",
            "Epoch 756, Loss: 3.897416606050985e-27\n",
            "Epoch 757, Loss: 3.659599695030138e-27\n",
            "Epoch 758, Loss: 3.3144607230443015e-27\n",
            "Epoch 759, Loss: 3.0994961263715758e-27\n",
            "Epoch 760, Loss: 2.7692469039717856e-27\n",
            "Epoch 761, Loss: 2.568592737157835e-27\n",
            "Epoch 762, Loss: 2.3629342339763882e-27\n",
            "Epoch 763, Loss: 2.1629210166479295e-27\n",
            "Epoch 764, Loss: 2.027471134031153e-27\n",
            "Epoch 765, Loss: 1.8575825425208216e-27\n",
            "Epoch 766, Loss: 1.6820979689640787e-27\n",
            "Epoch 767, Loss: 1.6023860396818243e-27\n",
            "Epoch 768, Loss: 1.4784855737555491e-27\n",
            "Epoch 769, Loss: 1.3488165624598453e-27\n",
            "Epoch 770, Loss: 1.2465481416689276e-27\n",
            "Epoch 771, Loss: 1.1627933002474155e-27\n",
            "Epoch 772, Loss: 1.0497273458162851e-27\n",
            "Epoch 773, Loss: 9.534493375243895e-28\n",
            "Epoch 774, Loss: 8.8303117578177e-28\n",
            "Epoch 775, Loss: 8.189855310391392e-28\n",
            "Epoch 776, Loss: 7.616082261359547e-28\n",
            "Epoch 777, Loss: 6.96970935714408e-28\n",
            "Epoch 778, Loss: 6.434270017725318e-28\n",
            "Epoch 779, Loss: 5.854333992871434e-28\n",
            "Epoch 780, Loss: 5.241241158094979e-28\n",
            "Epoch 781, Loss: 5.180474216489673e-28\n",
            "Epoch 782, Loss: 4.51585890384097e-28\n",
            "Epoch 783, Loss: 4.3167947847891055e-28\n",
            "Epoch 784, Loss: 3.876018753996865e-28\n",
            "Epoch 785, Loss: 3.6707916591229613e-28\n",
            "Epoch 786, Loss: 3.345263276202853e-28\n",
            "Epoch 787, Loss: 3.1098375998009575e-28\n",
            "Epoch 788, Loss: 2.66006362430854e-28\n",
            "Epoch 789, Loss: 2.481090806436523e-28\n",
            "Epoch 790, Loss: 2.3182649852182484e-28\n",
            "Epoch 791, Loss: 2.1481668525299678e-28\n",
            "Epoch 792, Loss: 2.107737731137391e-28\n",
            "Epoch 793, Loss: 1.8873497157412707e-28\n",
            "Epoch 794, Loss: 1.7163887664379046e-28\n",
            "Epoch 795, Loss: 1.549125602627762e-28\n",
            "Epoch 796, Loss: 1.4862632492429626e-28\n",
            "Epoch 797, Loss: 1.3934488333630529e-28\n",
            "Epoch 798, Loss: 1.2011639877154313e-28\n",
            "Epoch 799, Loss: 1.138178374814191e-28\n",
            "Epoch 800, Loss: 1.0009905330155995e-28\n",
            "Epoch 801, Loss: 9.572334046791215e-29\n",
            "Epoch 802, Loss: 8.811822830351583e-29\n",
            "Epoch 803, Loss: 7.861491958593146e-29\n",
            "Epoch 804, Loss: 8.127732514105237e-29\n",
            "Epoch 805, Loss: 7.543482406175925e-29\n",
            "Epoch 806, Loss: 7.23410101990956e-29\n",
            "Epoch 807, Loss: 6.297328694959608e-29\n",
            "Epoch 808, Loss: 6.04094890076278e-29\n",
            "Epoch 809, Loss: 5.827709937320225e-29\n",
            "Epoch 810, Loss: 5.114037337128091e-29\n",
            "Epoch 811, Loss: 4.402829927264772e-29\n",
            "Epoch 812, Loss: 3.8752791968982205e-29\n",
            "Epoch 813, Loss: 3.9689564293932156e-29\n",
            "Epoch 814, Loss: 3.717507015854018e-29\n",
            "Epoch 815, Loss: 3.125861336938259e-29\n",
            "Epoch 816, Loss: 3.244190472721411e-29\n",
            "Epoch 817, Loss: 2.9779499172093196e-29\n",
            "Epoch 818, Loss: 2.6722663164361775e-29\n",
            "Epoch 819, Loss: 2.150878561891665e-29\n",
            "Epoch 820, Loss: 2.2790684589900794e-29\n",
            "Epoch 821, Loss: 2.1459481812340337e-29\n",
            "Epoch 822, Loss: 1.874777245064311e-29\n",
            "Epoch 823, Loss: 1.829171223981221e-29\n",
            "Epoch 824, Loss: 1.8390319852964838e-29\n",
            "Epoch 825, Loss: 1.67632942359465e-29\n",
            "Epoch 826, Loss: 1.67632942359465e-29\n",
            "Epoch 827, Loss: 1.8944987676948362e-29\n",
            "Epoch 828, Loss: 1.8649164837490482e-29\n",
            "Epoch 829, Loss: 1.4902075537690676e-29\n",
            "Epoch 830, Loss: 1.3718784179859158e-29\n",
            "Epoch 831, Loss: 1.4026932970961116e-29\n",
            "Epoch 832, Loss: 1.2880619468061833e-29\n",
            "Epoch 833, Loss: 1.0661948172127738e-29\n",
            "Epoch 834, Loss: 9.429353007719907e-30\n",
            "Epoch 835, Loss: 9.380049201143593e-30\n",
            "Epoch 836, Loss: 9.380049201143593e-30\n",
            "Epoch 837, Loss: 9.380049201143593e-30\n",
            "Epoch 838, Loss: 9.675872040601473e-30\n",
            "Epoch 839, Loss: 8.887011135380461e-30\n",
            "Epoch 840, Loss: 8.887011135380461e-30\n",
            "Epoch 841, Loss: 7.95023881043051e-30\n",
            "Epoch 842, Loss: 7.753023584125257e-30\n",
            "Epoch 843, Loss: 7.543482406175925e-30\n",
            "Epoch 844, Loss: 6.803925307531227e-30\n",
            "Epoch 845, Loss: 6.323213193412173e-30\n",
            "Epoch 846, Loss: 6.12599796710692e-30\n",
            "Epoch 847, Loss: 5.485048481614848e-30\n",
            "Epoch 848, Loss: 5.485048481614848e-30\n",
            "Epoch 849, Loss: 4.190823558986625e-30\n",
            "Epoch 850, Loss: 4.190823558986625e-30\n",
            "Epoch 851, Loss: 4.190823558986625e-30\n",
            "Epoch 852, Loss: 4.190823558986625e-30\n",
            "Epoch 853, Loss: 4.190823558986625e-30\n",
            "Epoch 854, Loss: 4.190823558986625e-30\n",
            "Epoch 855, Loss: 5.17689969051289e-30\n",
            "Epoch 856, Loss: 4.190823558986625e-30\n",
            "Epoch 857, Loss: 4.190823558986625e-30\n",
            "Epoch 858, Loss: 4.190823558986625e-30\n",
            "Epoch 859, Loss: 4.190823558986625e-30\n",
            "Epoch 860, Loss: 4.190823558986625e-30\n",
            "Epoch 861, Loss: 4.190823558986625e-30\n",
            "Epoch 862, Loss: 4.338734978715565e-30\n",
            "Epoch 863, Loss: 4.338734978715565e-30\n",
            "Epoch 864, Loss: 4.338734978715565e-30\n",
            "Epoch 865, Loss: 4.338734978715565e-30\n",
            "Epoch 866, Loss: 4.190823558986625e-30\n",
            "Epoch 867, Loss: 4.190823558986625e-30\n",
            "Epoch 868, Loss: 4.190823558986625e-30\n",
            "Epoch 869, Loss: 4.190823558986625e-30\n",
            "Epoch 870, Loss: 4.6345578181734444e-30\n",
            "Epoch 871, Loss: 4.190823558986625e-30\n",
            "Epoch 872, Loss: 4.979684464207637e-30\n",
            "Epoch 873, Loss: 4.190823558986625e-30\n",
            "Epoch 874, Loss: 4.190823558986625e-30\n",
            "Epoch 875, Loss: 4.190823558986625e-30\n",
            "Epoch 876, Loss: 3.4512664603419266e-30\n",
            "Epoch 877, Loss: 3.4512664603419266e-30\n",
            "Epoch 878, Loss: 3.4512664603419266e-30\n",
            "Epoch 879, Loss: 3.4512664603419266e-30\n",
            "Epoch 880, Loss: 3.4512664603419266e-30\n",
            "Epoch 881, Loss: 3.4512664603419266e-30\n",
            "Epoch 882, Loss: 3.4512664603419266e-30\n",
            "Epoch 883, Loss: 3.4512664603419266e-30\n",
            "Epoch 884, Loss: 3.4512664603419266e-30\n",
            "Epoch 885, Loss: 2.8103169748498546e-30\n",
            "Epoch 886, Loss: 2.8103169748498546e-30\n",
            "Epoch 887, Loss: 2.8103169748498546e-30\n",
            "Epoch 888, Loss: 2.465190328815662e-30\n",
            "Epoch 889, Loss: 2.428212473883427e-30\n",
            "Epoch 890, Loss: 3.7594152514438844e-30\n",
            "Epoch 891, Loss: 3.069161959375499e-30\n",
            "Epoch 892, Loss: 3.069161959375499e-30\n",
            "Epoch 893, Loss: 2.428212473883427e-30\n",
            "Epoch 894, Loss: 2.2803010541544873e-30\n",
            "Epoch 895, Loss: 2.2803010541544873e-30\n",
            "Epoch 896, Loss: 2.2803010541544873e-30\n",
            "Epoch 897, Loss: 2.2803010541544873e-30\n",
            "Epoch 898, Loss: 2.2803010541544873e-30\n",
            "Epoch 899, Loss: 2.2803010541544873e-30\n",
            "Epoch 900, Loss: 2.2803010541544873e-30\n",
            "Epoch 901, Loss: 2.2803010541544873e-30\n",
            "Epoch 902, Loss: 2.2803010541544873e-30\n",
            "Epoch 903, Loss: 2.2803010541544873e-30\n",
            "Epoch 904, Loss: 2.2803010541544873e-30\n",
            "Epoch 905, Loss: 2.3789086673071137e-30\n",
            "Epoch 906, Loss: 2.3789086673071137e-30\n",
            "Epoch 907, Loss: 2.3789086673071137e-30\n",
            "Epoch 908, Loss: 2.3789086673071137e-30\n",
            "Epoch 909, Loss: 2.3789086673071137e-30\n",
            "Epoch 910, Loss: 2.3789086673071137e-30\n",
            "Epoch 911, Loss: 2.3789086673071137e-30\n",
            "Epoch 912, Loss: 2.3789086673071137e-30\n",
            "Epoch 913, Loss: 2.3789086673071137e-30\n",
            "Epoch 914, Loss: 1.6023737137301802e-30\n",
            "Epoch 915, Loss: 1.590047762086102e-30\n",
            "Epoch 916, Loss: 1.590047762086102e-30\n",
            "Epoch 917, Loss: 1.590047762086102e-30\n",
            "Epoch 918, Loss: 1.590047762086102e-30\n",
            "Epoch 919, Loss: 1.590047762086102e-30\n",
            "Epoch 920, Loss: 1.590047762086102e-30\n",
            "Epoch 921, Loss: 1.590047762086102e-30\n",
            "Epoch 922, Loss: 1.590047762086102e-30\n",
            "Epoch 923, Loss: 7.025792437124636e-31\n",
            "Epoch 924, Loss: 7.025792437124636e-31\n",
            "Epoch 925, Loss: 7.025792437124636e-31\n",
            "Epoch 926, Loss: 7.025792437124636e-31\n",
            "Epoch 927, Loss: 7.025792437124636e-31\n",
            "Epoch 928, Loss: 1.4914401489334754e-30\n",
            "Epoch 929, Loss: 7.025792437124636e-31\n",
            "Epoch 930, Loss: 7.025792437124636e-31\n",
            "Epoch 931, Loss: 7.025792437124636e-31\n",
            "Epoch 932, Loss: 7.025792437124636e-31\n",
            "Epoch 933, Loss: 7.025792437124636e-31\n",
            "Epoch 934, Loss: 1.4914401489334754e-30\n",
            "Epoch 935, Loss: 7.025792437124636e-31\n",
            "Epoch 936, Loss: 7.025792437124636e-31\n",
            "Epoch 937, Loss: 7.025792437124636e-31\n",
            "Epoch 938, Loss: 7.025792437124636e-31\n",
            "Epoch 939, Loss: 7.025792437124636e-31\n",
            "Epoch 940, Loss: 1.4914401489334754e-30\n",
            "Epoch 941, Loss: 7.025792437124636e-31\n",
            "Epoch 942, Loss: 7.025792437124636e-31\n",
            "Epoch 943, Loss: 7.025792437124636e-31\n",
            "Epoch 944, Loss: 7.025792437124636e-31\n",
            "Epoch 945, Loss: 7.025792437124636e-31\n",
            "Epoch 946, Loss: 1.4914401489334754e-30\n",
            "Epoch 947, Loss: 7.025792437124636e-31\n",
            "Epoch 948, Loss: 7.025792437124636e-31\n",
            "Epoch 949, Loss: 7.025792437124636e-31\n",
            "Epoch 950, Loss: 7.025792437124636e-31\n",
            "Epoch 951, Loss: 7.025792437124636e-31\n",
            "Epoch 952, Loss: 1.4914401489334754e-30\n",
            "Epoch 953, Loss: 7.025792437124636e-31\n",
            "Epoch 954, Loss: 7.025792437124636e-31\n",
            "Epoch 955, Loss: 7.025792437124636e-31\n",
            "Epoch 956, Loss: 7.025792437124636e-31\n",
            "Epoch 957, Loss: 7.025792437124636e-31\n",
            "Epoch 958, Loss: 1.4914401489334754e-30\n",
            "Epoch 959, Loss: 7.025792437124636e-31\n",
            "Epoch 960, Loss: 7.025792437124636e-31\n",
            "Epoch 961, Loss: 7.025792437124636e-31\n",
            "Epoch 962, Loss: 7.025792437124636e-31\n",
            "Epoch 963, Loss: 7.025792437124636e-31\n",
            "Epoch 964, Loss: 1.4914401489334754e-30\n",
            "Epoch 965, Loss: 7.025792437124636e-31\n",
            "Epoch 966, Loss: 7.025792437124636e-31\n",
            "Epoch 967, Loss: 7.025792437124636e-31\n",
            "Epoch 968, Loss: 7.025792437124636e-31\n",
            "Epoch 969, Loss: 7.025792437124636e-31\n",
            "Epoch 970, Loss: 1.4914401489334754e-30\n",
            "Epoch 971, Loss: 7.025792437124636e-31\n",
            "Epoch 972, Loss: 7.025792437124636e-31\n",
            "Epoch 973, Loss: 7.025792437124636e-31\n",
            "Epoch 974, Loss: 7.025792437124636e-31\n",
            "Epoch 975, Loss: 7.025792437124636e-31\n",
            "Epoch 976, Loss: 1.4914401489334754e-30\n",
            "Epoch 977, Loss: 7.025792437124636e-31\n",
            "Epoch 978, Loss: 7.025792437124636e-31\n",
            "Epoch 979, Loss: 7.025792437124636e-31\n",
            "Epoch 980, Loss: 7.025792437124636e-31\n",
            "Epoch 981, Loss: 7.025792437124636e-31\n",
            "Epoch 982, Loss: 1.4914401489334754e-30\n",
            "Epoch 983, Loss: 7.025792437124636e-31\n",
            "Epoch 984, Loss: 8.997944700177166e-31\n",
            "Epoch 985, Loss: 8.997944700177166e-31\n",
            "Epoch 986, Loss: 8.997944700177166e-31\n",
            "Epoch 987, Loss: 8.997944700177166e-31\n",
            "Epoch 988, Loss: 8.997944700177166e-31\n",
            "Epoch 989, Loss: 8.997944700177166e-31\n",
            "Epoch 990, Loss: 8.997944700177166e-31\n",
            "Epoch 991, Loss: 8.997944700177166e-31\n",
            "Epoch 992, Loss: 8.997944700177166e-31\n",
            "Epoch 993, Loss: 8.997944700177166e-31\n",
            "Epoch 994, Loss: 8.997944700177166e-31\n",
            "Epoch 995, Loss: 8.997944700177166e-31\n",
            "Epoch 996, Loss: 8.997944700177166e-31\n",
            "Epoch 997, Loss: 5.546678239835239e-31\n",
            "Epoch 998, Loss: 5.546678239835239e-31\n",
            "Epoch 999, Loss: 5.546678239835239e-31\n",
            "Epoch 1000, Loss: 2.2803010541544873e-30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ypred = [n(x) for x in xs]\n",
        "ypred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8GacaUKgOEh",
        "outputId": "8b7131b6-50a7-48db-c43a-57984c8b9903"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Value(data=0.9999999999999993),\n",
              " Value(data=-0.9999999999999999),\n",
              " Value(data=-1.0000000000000004),\n",
              " Value(data=0.9999999999999987)]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# beta = 1.0\n",
        "\n",
        "# xs = np.arange(-5, 5, 0.25)\n",
        "# ys = (1 / beta) * np.log(1 + np.exp(beta * xs))\n",
        "\n",
        "# plt.plot(xs, ys); plt.grid();"
      ],
      "metadata": {
        "id": "Y5W0P34zguRL"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3BSEW57HgvyQ"
      },
      "execution_count": 52,
      "outputs": []
    }
  ]
}